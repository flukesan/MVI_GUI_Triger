"""
AI Agent Module for MVI Inspection System
Supports multiple Ollama models with model selection
"""

import requests
import json
from datetime import datetime


class AIAgent:
    """AI Agent with model selection and chat capabilities"""

    def __init__(self, ollama_url="http://localhost:11434"):
        self.ollama_url = ollama_url
        self.current_model = "llama3.2"
        self.available_models = []
        self.conversation_history = []
        self.max_history = 10  # Keep last 10 exchanges

        # Try to connect and get available models
        self.refresh_available_models()

    def refresh_available_models(self):
        """Get list of installed models from Ollama"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                data = response.json()
                self.available_models = [
                    {
                        "name": model["name"],
                        "size": model.get("size", 0),
                        "modified": model.get("modified_at", ""),
                        "family": self.get_model_family(model["name"])
                    }
                    for model in data.get("models", [])
                ]
                print(f"‚úì ‡∏û‡∏ö {len(self.available_models)} AI models")

                # Set first available model as current if not set
                if self.available_models and self.current_model not in [m["name"] for m in self.available_models]:
                    self.current_model = self.available_models[0]["name"]
                    print(f"‚úì ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ default model: {self.current_model}")

                return True
            else:
                print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Ollama (status: {response.status_code})")
                return False
        except requests.exceptions.ConnectionError:
            print("‚ö†Ô∏è Ollama server ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô - ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏° Ollama ‡∏Å‡πà‡∏≠‡∏ô (ollama serve)")
            return False
        except Exception as e:
            print(f"‚ùå Error connecting to Ollama: {e}")
            return False

    def get_model_family(self, model_name):
        """Determine model family (text/vision/code)"""
        model_lower = model_name.lower()
        if "llava" in model_lower or "vision" in model_lower:
            return "vision"
        elif "code" in model_lower:
            return "code"
        else:
            return "text"

    def set_model(self, model_name):
        """Switch to different model"""
        if any(m["name"] == model_name for m in self.available_models):
            self.current_model = model_name
            print(f"‚úì ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô model: {model_name}")
            # Clear conversation history when switching models
            self.conversation_history = []
            return True
        else:
            print(f"‚ùå Model {model_name} ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö")
            return False

    def get_model_info(self, model_name=None):
        """Get detailed info about a model"""
        if model_name is None:
            model_name = self.current_model

        try:
            response = requests.post(
                f"{self.ollama_url}/api/show",
                json={"name": model_name},
                timeout=5
            )
            if response.status_code == 200:
                return response.json()
            else:
                return None
        except:
            return None

    def chat(self, user_message, context=None, system_prompt=None):
        """Chat with AI model"""
        if not self.available_models:
            return "‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö AI models - ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Ollama ‡πÅ‡∏•‡∏∞ pull models ‡∏Å‡πà‡∏≠‡∏ô"

        # Default system prompt for MVI context
        if system_prompt is None:
            system_prompt = """‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI Assistant ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö MVI (Maximo Visual Inspection).
‡∏Ñ‡∏∏‡∏ì‡∏ä‡πà‡∏ß‡∏¢‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö, ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤, ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•, ‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö.
‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô.
‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à ‡∏≠‡∏¢‡πà‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ó‡πá‡∏à."""

        # Build messages
        messages = [
            {"role": "system", "content": system_prompt}
        ]

        # Add context if provided
        if context:
            context_str = json.dumps(context, ensure_ascii=False, indent=2)
            messages.append({
                "role": "system",
                "content": f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°:\n{context_str}"
            })

        # Add conversation history (limit to max_history)
        messages.extend(self.conversation_history[-self.max_history:])

        # Add user message
        messages.append({"role": "user", "content": user_message})

        # Call Ollama API
        try:
            response = requests.post(
                f"{self.ollama_url}/api/chat",
                json={
                    "model": self.current_model,
                    "messages": messages,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "num_predict": 512
                    }
                },
                timeout=60  # 60 second timeout
            )

            if response.status_code == 200:
                ai_response = response.json()["message"]["content"]

                # Save to history
                self.conversation_history.append({"role": "user", "content": user_message})
                self.conversation_history.append({"role": "assistant", "content": ai_response})

                return ai_response
            else:
                return f"‚ùå Error: HTTP {response.status_code}"

        except requests.exceptions.Timeout:
            return "‚è±Ô∏è Timeout - AI ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà"
        except requests.exceptions.ConnectionError:
            return "‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Ollama server - ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏° Ollama ‡∏Å‡πà‡∏≠‡∏ô"
        except Exception as e:
            return f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}"

    def clear_history(self):
        """Clear conversation history"""
        self.conversation_history = []
        print("üóëÔ∏è ‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÅ‡∏•‡πâ‡∏ß")

    def analyze_defect_pattern(self, history_data):
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå pattern ‡∏Ç‡∏≠‡∏á defects ‡∏à‡∏≤‡∏Å history"""
        if not history_data:
            return "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå"

        prompt = f"""‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡∏∏‡∏õ:
1. ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà Pass/Fail
2. Defect ‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
3. Pattern ‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï
4. ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞

‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: device_id, result, count):
{json.dumps(history_data, ensure_ascii=False, indent=2)}

‡πÇ‡∏õ‡∏£‡∏î‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:"""

        return self.chat(prompt)

    def suggest_troubleshooting(self, error_description):
        """‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤"""
        prompt = f"""‡∏ä‡πà‡∏ß‡∏¢‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:

‡∏õ‡∏±‡∏ç‡∏´‡∏≤: {error_description}

‡πÇ‡∏õ‡∏£‡∏î‡πÉ‡∏´‡πâ:
1. ‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ
2. ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô
3. ‡∏ß‡∏¥‡∏ò‡∏µ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏î‡∏ã‡πâ‡∏≥"""

        return self.chat(prompt)

    def generate_summary_report(self, data, report_type="daily"):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ"""
        prompt = f"""‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ{report_type}‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:

{json.dumps(data, ensure_ascii=False, indent=2)}

‡πÇ‡∏õ‡∏£‡∏î‡∏™‡∏£‡∏∏‡∏õ‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö:
- ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏£‡∏ß‡∏° (Pass/Fail)
- ‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô/‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï
- ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞

‡πÉ‡∏ä‡πâ emoji ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢:"""

        return self.chat(prompt)

    def is_available(self):
        """Check if Ollama is available"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=2)
            return response.status_code == 200
        except:
            return False


# Example usage
if __name__ == "__main__":
    # Test AI Agent
    agent = AIAgent()

    if agent.is_available():
        print("\n=== Testing AI Agent ===\n")

        # Test chat
        response = agent.chat("‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ ‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡πà‡∏≠‡∏¢")
        print(f"AI: {response}\n")

        # Test defect analysis
        test_data = [
            ("Watashi_cam", "fail", 15),
            ("Watashi_cam", "pass", 5),
            ("Basler_GigE", "fail", 8),
            ("Basler_GigE", "pass", 12)
        ]
        analysis = agent.analyze_defect_pattern(test_data)
        print(f"Analysis: {analysis}\n")
    else:
        print("‚ùå Ollama not available. Please start Ollama server.")
