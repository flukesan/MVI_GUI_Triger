"""
Self-Reflection System (Level 3)
AI à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹à¸¥à¸°à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸„à¸³à¸•à¸­à¸šà¸‚à¸­à¸‡à¸•à¸±à¸§à¹€à¸­à¸‡

Features:
- Answer quality assessment
- Factual accuracy checking
- Hallucination detection
- Self-correction
"""

import json
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple


class SelfReflectionSystem:
    """AI reflects on and improves its own answers"""

    def __init__(self, ai_agent):
        self.ai_agent = ai_agent
        self.reflection_history = []

        print("âœ“ Self-Reflection System initialized")
        print("  â†’ Quality assessment: enabled")
        print("  â†’ Factual checking: enabled")
        print("  â†’ Self-correction: enabled")

    def reflect_on_answer(self,
                         question: str,
                         answer: str,
                         evidence: Dict[str, Any],
                         verbose: bool = False) -> Dict[str, Any]:
        """
        Reflect on an answer and assess its quality

        Args:
            question: Original question
            answer: Generated answer
            evidence: Evidence used to generate answer
            verbose: Print reflection process

        Returns:
            reflection: Dict with assessment and improvements
        """

        if verbose:
            print(f"\n{'â”'*60}")
            print(f"ðŸ” Self-Reflection Process")
            print(f"{'â”'*60}\n")

        # Step 1: Quality Assessment
        if verbose:
            print("1ï¸âƒ£ Quality Assessment...")
        quality = self._assess_quality(question, answer, evidence)

        # Step 2: Factual Accuracy Check
        if verbose:
            print("2ï¸âƒ£ Checking Factual Accuracy...")
        accuracy = self._check_accuracy(answer, evidence)

        # Step 3: Hallucination Detection
        if verbose:
            print("3ï¸âƒ£ Detecting Hallucinations...")
        hallucination = self._detect_hallucination(answer, evidence)

        # Step 4: Overall Assessment
        if verbose:
            print("4ï¸âƒ£ Overall Assessment...")

        overall_score = (quality['score'] + accuracy['score'] + (1.0 - hallucination['score'])) / 3.0

        reflection = {
            'timestamp': datetime.now().isoformat(),
            'question': question,
            'answer': answer[:200] + "..." if len(answer) > 200 else answer,
            'quality': quality,
            'accuracy': accuracy,
            'hallucination': hallucination,
            'overall_score': overall_score,
            'passed': overall_score >= 0.7,
            'improvements': []
        }

        # Step 5: Generate Improvements (if needed)
        if overall_score < 0.8:
            if verbose:
                print("5ï¸âƒ£ Generating Improvements...")
            improvements = self._generate_improvements(question, answer, evidence, reflection)
            reflection['improvements'] = improvements

        # Store reflection
        self.reflection_history.append(reflection)

        if verbose:
            print(f"\nðŸ“Š Reflection Score: {overall_score:.2f}")
            print(f"âœ… Passed: {reflection['passed']}")
            if reflection['improvements']:
                print(f"ðŸ’¡ Improvements suggested: {len(reflection['improvements'])}")
            print(f"{'â”'*60}\n")

        return reflection

    def _assess_quality(self, question: str, answer: str, evidence: Dict) -> Dict[str, Any]:
        """Assess answer quality"""

        prompt = f"""à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸„à¸¸à¸“à¸ à¸²à¸žà¸‚à¸­à¸‡à¸„à¸³à¸•à¸­à¸šà¸™à¸µà¹‰à¹ƒà¸™à¸¡à¸²à¸•à¸£à¸²à¸ªà¹ˆà¸§à¸™ 0-1:

à¸„à¸³à¸–à¸²à¸¡: {question}

à¸„à¸³à¸•à¸­à¸š: {answer}

à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸•à¸²à¸¡:
1. Relevance: à¸•à¸­à¸šà¸•à¸£à¸‡à¸›à¸£à¸°à¹€à¸”à¹‡à¸™à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ (0-1)
2. Completeness: à¸„à¸£à¸šà¸–à¹‰à¸§à¸™à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ (0-1)
3. Clarity: à¸­à¹ˆà¸²à¸™à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆà¸‡à¹ˆà¸²à¸¢à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ (0-1)
4. Structure: à¸ˆà¸±à¸”à¸£à¸¹à¸›à¹à¸šà¸šà¸”à¸µà¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ (0-1)

à¸•à¸­à¸šà¹ƒà¸™à¸£à¸¹à¸› JSON:
{{
  "relevance": 0.9,
  "completeness": 0.8,
  "clarity": 0.85,
  "structure": 0.9,
  "overall": 0.86,
  "reasoning": "à¸„à¸³à¸•à¸­à¸šà¸•à¸£à¸‡à¸›à¸£à¸°à¹€à¸”à¹‡à¸™ à¹à¸•à¹ˆà¸‚à¸²à¸”à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¸šà¸²à¸‡à¸ªà¹ˆà¸§à¸™"
}}

Assessment:"""

        try:
            response = self.ai_agent.chat(prompt)
            # Parse JSON
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                assessment = json.loads(json_match.group())
                return {
                    'score': assessment.get('overall', 0.5),
                    'details': assessment,
                    'reasoning': assessment.get('reasoning', '')
                }
        except:
            pass

        # Fallback: simple heuristic
        return {
            'score': 0.7,
            'details': {},
            'reasoning': 'Unable to assess automatically'
        }

    def _check_accuracy(self, answer: str, evidence: Dict) -> Dict[str, Any]:
        """Check if answer is factually accurate based on evidence"""

        # Extract claims from answer
        claims = self._extract_claims(answer)

        # Check each claim against evidence
        verified_claims = 0
        total_claims = len(claims)

        for claim in claims:
            if self._verify_claim(claim, evidence):
                verified_claims += 1

        accuracy_score = verified_claims / total_claims if total_claims > 0 else 1.0

        return {
            'score': accuracy_score,
            'verified_claims': verified_claims,
            'total_claims': total_claims,
            'details': claims[:3]  # Show first 3 claims
        }

    def _extract_claims(self, answer: str) -> List[str]:
        """Extract factual claims from answer"""

        # Simple extraction: split by periods and filter
        sentences = answer.split('.')
        claims = []

        for sentence in sentences:
            sentence = sentence.strip()
            # Look for sentences with numbers or specific assertions
            if sentence and (any(char.isdigit() for char in sentence) or len(sentence.split()) > 5):
                claims.append(sentence)

        return claims[:10]  # Limit to 10 claims

    def _verify_claim(self, claim: str, evidence: Dict) -> bool:
        """Verify if a claim is supported by evidence"""

        # Convert evidence to text
        evidence_text = json.dumps(evidence, ensure_ascii=False)

        # Check if claim keywords appear in evidence
        claim_words = set(claim.lower().split())
        evidence_words = set(evidence_text.lower().split())

        # Calculate overlap
        overlap = len(claim_words & evidence_words)
        overlap_ratio = overlap / len(claim_words) if claim_words else 0

        # Consider verified if >30% overlap
        return overlap_ratio > 0.3

    def _detect_hallucination(self, answer: str, evidence: Dict) -> Dict[str, Any]:
        """Detect potential hallucinations (made-up facts)"""

        prompt = f"""à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¸„à¸³à¸•à¸­à¸šà¸™à¸µà¹‰à¸¡à¸µà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹à¸•à¹ˆà¸‡à¸‚à¸¶à¹‰à¸™à¸¡à¸²à¹€à¸­à¸‡ (hallucination) à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ:

à¸„à¸³à¸•à¸­à¸š: {answer}

à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸—à¸µà¹ˆà¸¡à¸µ:
{json.dumps(evidence, ensure_ascii=False, indent=2, default=str)[:500]}

à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š:
1. à¸¡à¸µà¸•à¸±à¸§à¹€à¸¥à¸‚à¸«à¸£à¸·à¸­à¸Šà¸·à¹ˆà¸­à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸¡à¸µà¹ƒà¸™à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ?
2. à¸¡à¸µà¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¸—à¸µà¹ˆà¹€à¸‰à¸žà¸²à¸°à¹€à¸ˆà¸²à¸°à¸ˆà¸‡à¹€à¸à¸´à¸™à¹„à¸› à¹à¸•à¹ˆà¹„à¸¡à¹ˆà¸¡à¸µà¹ƒà¸™à¸«à¸¥à¸±à¸à¸à¸²à¸™?
3. à¸¡à¸µà¸‚à¹‰à¸­à¹€à¸—à¹‡à¸ˆà¸ˆà¸£à¸´à¸‡à¸—à¸µà¹ˆà¸‚à¸±à¸”à¹à¸¢à¹‰à¸‡à¸à¸±à¸šà¸«à¸¥à¸±à¸à¸à¸²à¸™?

à¸•à¸­à¸šà¹ƒà¸™ JSON:
{{
  "has_hallucination": false,
  "confidence": 0.95,
  "suspicious_parts": [],
  "reasoning": "à¸„à¸³à¸•à¸­à¸šà¸­à¸´à¸‡à¸ˆà¸²à¸à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”"
}}

Analysis:"""

        try:
            response = self.ai_agent.chat(prompt)
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                return {
                    'score': 1.0 if result.get('has_hallucination', False) else 0.0,
                    'confidence': result.get('confidence', 0.5),
                    'suspicious_parts': result.get('suspicious_parts', []),
                    'reasoning': result.get('reasoning', '')
                }
        except:
            pass

        # Fallback: simple check
        return {
            'score': 0.0,
            'confidence': 0.5,
            'suspicious_parts': [],
            'reasoning': 'Unable to detect automatically'
        }

    def _generate_improvements(self, question: str, answer: str, evidence: Dict, reflection: Dict) -> List[str]:
        """Generate improvement suggestions"""

        issues = []

        # Identify issues
        if reflection['quality']['score'] < 0.7:
            issues.append(f"Quality issues: {reflection['quality']['reasoning']}")

        if reflection['accuracy']['score'] < 0.8:
            issues.append(f"Accuracy issues: Only {reflection['accuracy']['verified_claims']}/{reflection['accuracy']['total_claims']} claims verified")

        if reflection['hallucination']['score'] > 0.3:
            issues.append(f"Possible hallucinations: {', '.join(reflection['hallucination']['suspicious_parts'][:2])}")

        prompt = f"""à¸„à¸³à¸•à¸­à¸šà¸¡à¸µà¸›à¸±à¸à¸«à¸²à¸”à¸±à¸‡à¸™à¸µà¹‰:
{chr(10).join(['- ' + issue for issue in issues])}

à¸„à¸³à¸–à¸²à¸¡: {question}
à¸„à¸³à¸•à¸­à¸šà¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™: {answer[:200]}...
à¸«à¸¥à¸±à¸à¸à¸²à¸™: {json.dumps(evidence, ensure_ascii=False, default=str)[:300]}...

à¹à¸™à¸°à¸™à¸³à¸§à¸´à¸˜à¸µà¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡ 3 à¸‚à¹‰à¸­:

Suggestions:"""

        try:
            response = self.ai_agent.chat(prompt)
            # Extract bullet points
            import re
            suggestions = re.findall(r'[â€¢\-\d]+\.\s*(.+)', response)
            return suggestions[:3]
        except:
            return ["à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸„à¸§à¸²à¸¡à¸–à¸¹à¸à¸•à¹‰à¸­à¸‡à¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥", "à¹€à¸žà¸´à¹ˆà¸¡à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¸ˆà¸²à¸à¸«à¸¥à¸±à¸à¸à¸²à¸™", "à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¹à¸™à¹ˆà¹ƒà¸ˆ"]

    def get_corrected_answer(self, question: str, answer: str, evidence: Dict) -> Tuple[str, Dict]:
        """
        Get self-corrected answer

        Args:
            question: Original question
            answer: Original answer
            evidence: Evidence available

        Returns:
            (corrected_answer, reflection)
        """

        # First reflect
        reflection = self.reflect_on_answer(question, answer, evidence, verbose=False)

        # If score is high enough, no need to correct
        if reflection['overall_score'] >= 0.8:
            return answer, reflection

        # Generate corrected answer
        prompt = f"""à¸„à¸³à¸•à¸­à¸šà¹€à¸”à¸´à¸¡à¸¡à¸µà¸›à¸±à¸à¸«à¸² (à¸„à¸°à¹à¸™à¸™: {reflection['overall_score']:.2f}/1.0):

à¸„à¸³à¸–à¸²à¸¡: {question}

à¸„à¸³à¸•à¸­à¸šà¹€à¸”à¸´à¸¡:
{answer}

à¸›à¸±à¸à¸«à¸²:
- Quality: {reflection['quality']['reasoning']}
- Accuracy: {reflection['accuracy']['verified_claims']}/{reflection['accuracy']['total_claims']} claims verified
- Hallucination: {reflection['hallucination']['reasoning']}

à¸«à¸¥à¸±à¸à¸à¸²à¸™à¸—à¸µà¹ˆà¸¡à¸µ:
{json.dumps(evidence, ensure_ascii=False, indent=2, default=str)}

à¸‚à¹‰à¸­à¹€à¸ªà¸™à¸­à¹à¸™à¸°:
{chr(10).join(['- ' + imp for imp in reflection['improvements']])}

à¹‚à¸›à¸£à¸”à¸ªà¸£à¹‰à¸²à¸‡à¸„à¸³à¸•à¸­à¸šà¹ƒà¸«à¸¡à¹ˆà¸—à¸µà¹ˆ:
1. à¸­à¸´à¸‡à¸ˆà¸²à¸à¸«à¸¥à¸±à¸à¸à¸²à¸™à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™ (à¹„à¸¡à¹ˆà¹à¸•à¹ˆà¸‡à¹€à¸žà¸´à¹ˆà¸¡)
2. à¸•à¸­à¸šà¸•à¸£à¸‡à¸›à¸£à¸°à¹€à¸”à¹‡à¸™ à¸„à¸£à¸šà¸–à¹‰à¸§à¸™
3. à¸­à¹ˆà¸²à¸™à¸‡à¹ˆà¸²à¸¢ à¸¡à¸µà¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¸Šà¸±à¸”à¹€à¸ˆà¸™

à¸„à¸³à¸•à¸­à¸šà¸—à¸µà¹ˆà¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡à¹à¸¥à¹‰à¸§:"""

        corrected_answer = self.ai_agent.chat(prompt)

        # Reflect again
        new_reflection = self.reflect_on_answer(question, corrected_answer, evidence, verbose=False)
        new_reflection['is_corrected'] = True
        new_reflection['original_score'] = reflection['overall_score']
        new_reflection['improvement'] = new_reflection['overall_score'] - reflection['overall_score']

        return corrected_answer, new_reflection

    def get_reflection_summary(self) -> Dict[str, Any]:
        """Get summary of all reflections"""

        if not self.reflection_history:
            return {
                'total_reflections': 0,
                'average_score': 0.0,
                'pass_rate': 0.0
            }

        total = len(self.reflection_history)
        total_score = sum(r['overall_score'] for r in self.reflection_history)
        passed = sum(1 for r in self.reflection_history if r['passed'])

        return {
            'total_reflections': total,
            'average_score': total_score / total,
            'pass_rate': passed / total,
            'recent_scores': [r['overall_score'] for r in self.reflection_history[-5:]]
        }

    def clear_history(self):
        """Clear reflection history"""
        self.reflection_history = []
        print("âœ“ Reflection history cleared")


# Example usage
if __name__ == "__main__":
    print("Self-Reflection System (Level 3)")
    print("=" * 60)
    print("\nFeatures:")
    print("  âœ“ Quality assessment (relevance, completeness, clarity)")
    print("  âœ“ Factual accuracy checking")
    print("  âœ“ Hallucination detection")
    print("  âœ“ Self-correction")
    print("=" * 60)
